{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavenet Power Law Decoding\n",
    "\n",
    "How populations of neurons in the visual cortex encode information is an open question. Some research has demonstrated that they maintain a balance between a high-dimensional, uncorrelated representation (highly flexible, but susceptible to noise) and a low-dimensional, correlated representation (less flexible, but robust to noise) [Stringer, et al., 2019](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6642054/).\n",
    "\n",
    "Additional research has found that natural images are encoded by a sparse amount of neurons. [Yoshida, Ohki, 2020](https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32054847/) demonstrated that a single natural image is linearly decodable from a surprisingly small number of highly responsive neurons (~20), and the remaining neurons even degrade decoding. \n",
    "\n",
    "Do these properties hold for behavioral signals encoded in the visual cortex?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3a0e06cf6d58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbehavenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_best_model_and_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbehavenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexport_predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbehavenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/behavenet/behavenet/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbehavenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConditionalAE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAEMSP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbehavenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCustomDataParallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbehavenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConvDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbehavenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvaes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVAE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConditionalVAE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBetaTCVAE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSSSVAE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/behavenet/behavenet/models/aes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/behavenet/lib/python3.5/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0m_dl_flags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m __all__ += [name for name in dir(_C)\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 18, 'figure.figsize': (16, 8)})\n",
    "\n",
    "import behavenet\n",
    "from behavenet import get_user_dir, make_dir_if_not_exists\n",
    "from behavenet.data.utils import get_transforms_paths\n",
    "from behavenet.data.utils import load_labels_like_latents\n",
    "from behavenet.fitting.utils import get_expt_dir\n",
    "from behavenet.fitting.utils import get_session_dir\n",
    "from behavenet.fitting.utils import get_best_model_version\n",
    "from behavenet.fitting.utils import get_lab_example\n",
    "\n",
    "from behavenet.fitting.utils import get_best_model_and_data\n",
    "from behavenet.fitting.eval import export_predictions\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from behavenet.fitting.utils import get_subdirs\n",
    "import os\n",
    "          \n",
    "def latent_results(expt_name=None, sess_id='', version='best'):\n",
    "    # set model info\n",
    "    sess_idx = 0\n",
    "    hparams = {\n",
    "        'data_dir': get_user_dir('data'),\n",
    "        'save_dir': get_user_dir('save'),\n",
    "        'model_class': 'neural-ae',\n",
    "        'ae_model_type': 'conv',\n",
    "        'ae_experiment_name': 'latent_search',\n",
    "        'n_ae_latents': 9,\n",
    "        'experiment_name':'grid_search',\n",
    "        'model_type':'ff',\n",
    "        'n_max_lags': 8,\n",
    "        'rng_seed_data': 0,\n",
    "        'trial_splits': '8;1;1;0'\n",
    "    }\n",
    "\n",
    "    hparams['neural_ae_experiment_name'] = hparams['experiment_name']\n",
    "    hparams['neural_ae_model_type'] = hparams['model_type']\n",
    "    hparams['neural_ae_version'] = version\n",
    "    \n",
    "    get_lab_example(hparams, 'dipoppa', sess_id)\n",
    "    \n",
    "    hparams['session_dir'], sess_ids = get_session_dir(hparams)\n",
    "    expt_dir = get_expt_dir(hparams)\n",
    "\n",
    "    ## Get discrete chance performance (accuracy of always predicting the most common training state)\n",
    "    _, latents_file = get_transforms_paths('ae_latents', hparams, sess_ids[sess_idx])\n",
    "    with open(latents_file, 'rb') as f:\n",
    "        all_latents = pickle.load(f)\n",
    "    mean_ae_latents = np.mean(np.concatenate([all_latents['latents'][i] for i in all_latents['trials']['train']]),axis=0)\n",
    "\n",
    "    all_test_latents = np.concatenate([all_latents['latents'][i][hparams['n_max_lags']:-hparams['n_max_lags']] for i in all_latents['trials']['test']])\n",
    "    chance_ae_performance = np.mean((all_test_latents-mean_ae_latents)**2)\n",
    "    \n",
    "    ## Get discrete prediction performance\n",
    "    # If sub-sampling - make sure to get results labelled by sample method and index name \n",
    "    if expt_name is not None:\n",
    "        hparams['subsample_method'] = 'single'\n",
    "        hparams['subsample_idxs_name'] = expt_name\n",
    "        \n",
    "    \n",
    "    _, latent_predictions_file = get_transforms_paths('neural_ae_predictions', hparams, sess_ids[sess_idx])\n",
    "\n",
    "    if not os.path.exists(latent_predictions_file):\n",
    "        model, data_generator = get_best_model_and_data(hparams, Decoder, load_data=True, version=version)\n",
    "        predictions = export_predictions(data_generator, model)\n",
    "        \n",
    "    with open(latent_predictions_file, 'rb') as f:\n",
    "        all_latent_predictions = pickle.load(f)\n",
    "    all_test_latent_predictions = np.concatenate([all_latent_predictions['predictions'][i][hparams['n_max_lags']:-hparams['n_max_lags']] for i in all_latents['trials']['test']])\n",
    "    decoding_ae_performance = np.mean((all_test_latents-all_test_latent_predictions)**2)\n",
    "    r2 = r2_score(all_test_latents, all_test_latent_predictions)\n",
    "    \n",
    "    return chance_ae_performance, decoding_ae_performance, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [20, 60, 80, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300]\n",
    "sess_ids = ['MD0ST5_1', 'MD0ST5_2', 'MD0ST5_3', 'MD0ST5_4']\n",
    "decoding_errors = {}\n",
    "\n",
    "failures = []\n",
    "for sess_id in sess_ids:\n",
    "    decoding_errors[sess_id] = {}\n",
    "    for sample_size in samples:\n",
    "        decoding_errors[sess_id][sample_size] = []\n",
    "        for i in range(10):\n",
    "            expt_name = 'n{}_t{}'.format(sample_size, i)\n",
    "            try:\n",
    "                chance, decoding, r2 = latent_results(expt_name=expt_name, sess_id=sess_id)\n",
    "                decoding_errors[sess_id][sample_size].append((chance, decoding, r2))\n",
    "            except:\n",
    "                print('Failure on session: ', sess_id, 'trial: ',expt_name)\n",
    "                failures.append(expt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(res, session, samples, signal):\n",
    "    '''\n",
    "    Params:\n",
    "        res: dict of results (from above cell)\n",
    "        session: Session ID in dict\n",
    "        samples: array of # neurons used in decoding\n",
    "        signal: 'r2' | 'mse' | 'chance' --> what value to plot\n",
    "    '''\n",
    "    \n",
    "    idx = 0\n",
    "    if signal == 'r2':\n",
    "        idx = 2\n",
    "    elif signal == 'mse':\n",
    "        idx = 1\n",
    "    elif signal == 'chance':\n",
    "        idx = 0\n",
    "    else:\n",
    "        raise ValueError('The only signals available are r2, mse, and chance')\n",
    "        \n",
    "    ret = []\n",
    "    vals = res[session]\n",
    "    \n",
    "    # Iterate through dict of results and gather relevant metric\n",
    "    for sample in samples:\n",
    "        sample_vals = []\n",
    "        for trial in vals[sample]:\n",
    "            sample_vals.append(trial[idx])\n",
    "        ret.append(sample_vals)\n",
    "        \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import sem\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_yscale('log', basey=10)\n",
    "ax.set_xscale('log', basex=10)\n",
    "\n",
    "plt.grid = True\n",
    "\n",
    "st = 0 # where to start on the x axis (ie; to start plot from 100 neurons, set st = 3)\n",
    "for i, sess in enumerate(sess_ids):\n",
    "    res = get_results(decoding_errors, sess, samples, 'mse')\n",
    "    chances = get_results(decoding_errors, sess, samples, 'chance')\n",
    "    chance = chances[0][0]\n",
    "        \n",
    "    means = [np.mean(t) for t in res[st:]] / chance\n",
    "    std_error = [sem(t) for t in res[st:]] / chance\n",
    "    \n",
    "    sns.lineplot(samples[st:], means, label='Session %d'%(i+1))\n",
    "    plt.fill_between(samples[st:], means+std_error, means-std_error, alpha=0.3)\n",
    "    \n",
    "\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xlabel('Number of Neurons')\n",
    "\n",
    "plt.axhline(1, 0.05, 0.95, color='black', linewidth=4, label='Chance Performance')\n",
    "plt.legend()\n",
    "plt.title('[MD0ST5, Sessions: 1-4] Decoding 9 Non-Linear Latent Variables')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "behavenet",
   "language": "python",
   "name": "behavenet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
